{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a1aa87-f5bf-4c96-8388-7180d44d1223",
   "metadata": {},
   "source": [
    "## RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d17f93d-916d-44c3-bd15-b4191683e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from operator import add as add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import Image, display\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", temperature = 0) # To minimize hallucination - temperature = 0 makes the model output more deterministic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e68823-b997-44f9-b366-28e1a38ae964",
   "metadata": {},
   "source": [
    "### Embedding Model - has to also be compatible with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a46e1a-d29d-4624-854d-6bf89ecfd6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has been loaded and has 9 pages\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",)\n",
    "\n",
    "pdf_path = \"Stock_Market_Performance_2024.pdf\"\n",
    "\n",
    "# Safety measure for debugging purposes \n",
    "if not os.path.exists(pdf_path):\n",
    "    raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "pdf_loader = PyPDFLoader(pdf_path) \n",
    "\n",
    "# Checks if the PDF is there\n",
    "try:\n",
    "    pages = pdf_loader.load()\n",
    "    print(f\"PDF has been loaded and has {len(pages)} pages\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF: {e}\")\n",
    "    raise\n",
    "\n",
    "# Chunking \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "pages_split = text_splitter.split_documents(pages) # We now apply this to our pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814cee19-a1ea-4232-b4c4-5e888636985f",
   "metadata": {},
   "source": [
    "### Create tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1e81b9-40e3-495f-bc9d-389b250182cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ChromaDB vector store!\n"
     ]
    }
   ],
   "source": [
    "persist_directory = r\"C:\\Users\\MILTON\\Documents\\Training\\data\"\n",
    "collection_name = \"stock_market\"\n",
    "\n",
    "# If our collection does not exist in the directory, we create using the os command\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Here, we actually create the chroma database using our embeddigns model\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=pages_split,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    print(f\"Created ChromaDB vector store!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error setting up ChromaDB: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Now we create our retriever \n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5} # K is the amount of chunks to return\n",
    ")\n",
    "\n",
    "@tool\n",
    "def retriever_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool searches and returns the information from the Stock Market Performance 2024 document.\n",
    "    \"\"\"\n",
    "\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    if not docs:\n",
    "        return \"I found no relevant information in the Stock Market Performance 2024 document.\"\n",
    "    \n",
    "    results = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        results.append(f\"Document {i+1}:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "\n",
    "tools = [retriever_tool]\n",
    "\n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "def Should_continue(state: AgentState):\n",
    "    \"\"\"Check if the last message contains tool calls.\"\"\"\n",
    "    result = state['messages'][-1]\n",
    "    return hasattr(result, 'tool_calls') and len(result.tool_calls) > 0\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an intelligent AI assistant who answers questions about Stock Market Performance in 2024 based on the PDF document loaded into your knowledge base.\n",
    "Use the retriever tool available to answer questions about the stock market performance data. You can make multiple calls if needed.\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "Please always cite the specific parts of the documents you use in your answers.\n",
    "\"\"\"\n",
    "\n",
    "# Creating a dictionary of our tools\n",
    "tools_dict = {our_tool.name: our_tool for our_tool in tools} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d344a2b-8764-47b6-9266-dc5b1d7bef86",
   "metadata": {},
   "source": [
    "### Create agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4654c916-ffab-4412-a8c4-d357b710ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Agent\n",
    "def Call_llm(state: AgentState) -> AgentState:\n",
    "    \"\"\"Function to call the LLM with the current state.\"\"\"\n",
    "    messages = list(state['messages'])\n",
    "    messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    message = llm.invoke(messages)\n",
    "    return {'messages': [message]}\n",
    "\n",
    "\n",
    "# Retriever Agent\n",
    "def Retriever(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute tool calls from the LLM's response.\"\"\"\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    results = []\n",
    "    for t in tool_calls:\n",
    "        print(f\"Calling Tool: {t['name']} with query: {t['args'].get('query', 'No query provided')}\")\n",
    "        \n",
    "        if not t['name'] in tools_dict: # Checks if a valid tool is present\n",
    "            print(f\"\\nTool: {t['name']} does not exist.\")\n",
    "            result = \"Incorrect Tool Name, Please Retry and Select tool from List of Available tools.\"\n",
    "        \n",
    "        else:\n",
    "            result = tools_dict[t['name']].invoke(t['args'].get('query', ''))\n",
    "            print(f\"Result length: {len(str(result))}\")\n",
    "            \n",
    "\n",
    "        # Appends the Tool Message\n",
    "        results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "\n",
    "    print(\"Tools Execution Complete. Back to the model!\")\n",
    "    return {'messages': results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4f750-afb5-4ab2-aae4-defd84b6bfdb",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd9dc7f-fa5f-40ed-b5eb-65a94572eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"call_llm\", Call_llm)\n",
    "graph.add_node(\"retriever\", Retriever)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"call_llm\",\n",
    "    Should_continue,\n",
    "    {True: \"retriever\", False: END}\n",
    ")\n",
    "graph.add_edge(\"retriever\", \"call_llm\")\n",
    "graph.set_entry_point(\"call_llm\")\n",
    "\n",
    "rag_agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a48a2f-3430-48d2-bb77-21b50adb1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG AGENT===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What is your question:  how many messages?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Tool: retriever_tool with query: number of messages\n",
      "Result length: 4185\n",
      "Tools Execution Complete. Back to the model!\n",
      "\n",
      "=== ANSWER ===\n",
      "The document does not provide a specific count of messages. It primarily discusses stock market performance in 2024, focusing on various companies and their financial metrics. If you have a specific question about the stock market performance or any related topic, feel free to ask!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What is your question:  exit\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== RAG AGENT===\")\n",
    "    \n",
    "while True:\n",
    "    user_input = input(\"\\nWhat is your question: \")\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "            \n",
    "    messages = [HumanMessage(content=user_input)] # converts back to a HumanMessage type\n",
    "\n",
    "    result = rag_agent.invoke({\"messages\": messages})\n",
    "        \n",
    "    print(\"\\n=== ANSWER ===\")\n",
    "    print(result['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
